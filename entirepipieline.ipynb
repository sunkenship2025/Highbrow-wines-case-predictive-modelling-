{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7831289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    FunctionTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "#import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69205a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd966bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our data\n",
    "data = pd.read_csv('data_2016.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "243a522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets train test split\n",
    "X = data.drop(columns=['bought_highbrow_wines'])\n",
    "y = data['bought_highbrow_wines']\n",
    "#lets drop x values whose y is nan\n",
    "X = X.loc[y.dropna().index]\n",
    "y = y.dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded343f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    feature set based on behavioral relevance:\n",
    "    - Intent signals (wine/premium purchase history)\n",
    "    - Ability signals (spend capacity)\n",
    "    - Willingness signals (price sensitivity, discount behavior)\n",
    "    - Channel readiness (online purchase patterns)\n",
    "    - Stable context (household, loyalty)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Identify all category columns for aggregations\n",
    "    # =========================================================================\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"cat_\")]\n",
    "    \n",
    "    # Coerce all cat_* to numeric\n",
    "    for c in cat_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Calculate TOTAL_SPEND (sum of all category purchases)\n",
    "    # =========================================================================\n",
    "    df[\"total_spend\"] = df[cat_cols].sum(axis=1)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: ONLINE CHANNEL FEATURES\n",
    "    # =========================================================================\n",
    "    # Clean online metrics\n",
    "    df[\"n_cogo\"] = pd.to_numeric(df.get(\"n_cogo\", 0), errors=\"coerce\").fillna(0)\n",
    "    df[\"cogo_rev\"] = pd.to_numeric(df.get(\"cogo_rev\", 0), errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    # Online ratio: what fraction of spend is online?\n",
    "    df[\"online_ratio\"] = np.where(\n",
    "        df[\"total_spend\"] > 0,\n",
    "        df[\"cogo_rev\"] / df[\"total_spend\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: DISCOUNT BEHAVIOR\n",
    "    # =========================================================================\n",
    "    df[\"total_discount\"] = pd.to_numeric(df.get(\"total_discount\", 0), errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    # Discount ratio: how deal-dependent is this customer?\n",
    "    df[\"discount_ratio\"] = np.where(\n",
    "        df[\"total_spend\"] > 0,\n",
    "        df[\"total_discount\"] / df[\"total_spend\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: WINE AFFINITY FEATURES\n",
    "    # =========================================================================\n",
    "    # Wine-adjacent premium foods that signal taste alignment\n",
    "    wine_affinity_cols = [\n",
    "    \"cat_Wijn_Stillewijnen_RAYON\",     # Still wines (anchor)\n",
    "    \"cat_Tapas\",                      # Wine-paired appetizers\n",
    "    \"cat_KaasSeizoenskazen\",           # Seasonal / specialty cheeses\n",
    "    \"cat_VerseKaasFruitkazen\",         # Fresh / fruit cheeses\n",
    "    \"cat_VisGerookt\",                  # Smoked fish\n",
    "    \"cat_VisVerseSchelpdieren\",        # Fresh fish & shellfish\n",
    "]\n",
    "\n",
    "    wine_affinity_cols = [c for c in wine_affinity_cols if c in df.columns]\n",
    "    \n",
    "    df[\"wine_affinity_spend\"] = df[wine_affinity_cols].sum(axis=1) if wine_affinity_cols else 0\n",
    "    \n",
    "    # Wine affinity ratio: lifestyle vs transactional\n",
    "    df[\"wine_affinity_ratio\"] = np.where(\n",
    "        df[\"total_spend\"] > 0,\n",
    "        df[\"wine_affinity_spend\"] / df[\"total_spend\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: PREMIUM VS NECESSITY RATIO\n",
    "    # =========================================================================\n",
    "    # Premium lifestyle categories (discretionary, taste-driven)\n",
    "    premium_cols = [\n",
    "    \"cat_Wijn_Stillewijnen_RAYON\",   # Premium anchor\n",
    "    \"cat_Tapas\",                    # Gourmet food\n",
    "    \"cat_KaasSeizoenskazen\",         # Specialty cheese\n",
    "    \"cat_VisGerookt\",               # Premium fish\n",
    "    \"cat_Bier_Genietbieren\",         # Craft / premium beers\n",
    "    \"cat_Bloemen\",                  # Gifting / discretionary\n",
    "    \"cat_ParfumerieEHBO\",            # Personal care / premium\n",
    "    \"cat_Textiel_Bedlinnen\",         # Lifestyle / home comfort\n",
    "]\n",
    "    premium_cols = [c for c in premium_cols if c in df.columns]\n",
    "    \n",
    "    # Necessity categories (survival shopping, family logistics)\n",
    "    necessity_cols = [\n",
    "        \"cat_Babyluiers\",                # Baby diapers\n",
    "        \"cat_Incontinentie_luiers\",      # Adult diapers\n",
    "        \"cat_MelkKarnemelk\",             # Basic dairy\n",
    "        \"cat_BroodKorthoudbaar\",         # Bread staples\n",
    "        \"cat_Bot_Mar_Boter\",             # Butter (basic)\n",
    "    ]\n",
    "    necessity_cols = [c for c in necessity_cols if c in df.columns]\n",
    "    \n",
    "    premium_spend = df[premium_cols].sum(axis=1) if premium_cols else 0\n",
    "    necessity_spend = df[necessity_cols].sum(axis=1) if necessity_cols else 0\n",
    "    \n",
    "    # Premium ratio: lifestyle orientation vs survival shopping\n",
    "    df[\"premium_ratio\"] = np.where(\n",
    "        (premium_spend + necessity_spend) > 0,\n",
    "        premium_spend / (premium_spend + necessity_spend),\n",
    "        0.5  # Neutral if no signal\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7: CLEAN OTHER NUMERIC FEATURES\n",
    "    # =========================================================================\n",
    "    df[\"rev_ticket\"] = pd.to_numeric(df.get(\"rev_ticket\", 0), errors=\"coerce\").fillna(0)\n",
    "    df[\"prod_ticket\"] = pd.to_numeric(df.get(\"prod_ticket\", 0), errors=\"coerce\").fillna(0)\n",
    "    df[\"price_sens_colr\"] = pd.to_numeric(df.get(\"price_sens_colr\", 0), errors=\"coerce\").fillna(0)\n",
    "    df[\"SOW_colr\"] = pd.to_numeric(df.get(\"SOW_colr\", 0), errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    # Keep the wine anchor feature directly\n",
    "    df[\"cat_Wijn_Stillewijnen_RAYON\"] = pd.to_numeric(\n",
    "        df.get(\"cat_Wijn_Stillewijnen_RAYON\", 0), errors=\"coerce\"\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 8: HANDLE CATEGORICAL FEATURES\n",
    "    # =========================================================================\n",
    "    # HOUSEHOLDTYPOLOGY: normalize \"!\" to \"unknown\"\n",
    "    if \"HOUSEHOLDTYPOLOGY\" in df.columns:\n",
    "        df[\"HOUSEHOLDTYPOLOGY\"] = (\n",
    "            df[\"HOUSEHOLDTYPOLOGY\"]\n",
    "            .fillna(\"unknown\")\n",
    "            .replace(\"!\", \"unknown\")\n",
    "            .astype(str)\n",
    "        )\n",
    "    \n",
    "    # SOW_type_colr: handle outlier flags\n",
    "    if \"SOW_type_colr\" in df.columns:\n",
    "        df[\"SOW_type_colr\"] = (\n",
    "            df[\"SOW_type_colr\"]\n",
    "            .fillna(\"unknown\")\n",
    "            .replace(\"!\", \"unknown\")\n",
    "            .astype(str)\n",
    "        )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: SELECT FINAL FEATURE SET\n",
    "    # =========================================================================\n",
    "    final_numeric = [\n",
    "        \"total_spend\",\n",
    "        \"rev_ticket\",\n",
    "        \"prod_ticket\",\n",
    "        \"n_cogo\",\n",
    "        \"cogo_rev\",\n",
    "        \"online_ratio\",\n",
    "        \"price_sens_colr\",\n",
    "        \"discount_ratio\",\n",
    "        \"cat_Wijn_Stillewijnen_RAYON\",\n",
    "        \"wine_affinity_spend\",\n",
    "        \"wine_affinity_ratio\",\n",
    "        \"premium_ratio\",\n",
    "        \"SOW_colr\",\n",
    "    ]\n",
    "    \n",
    "    final_categorical = [\n",
    "        \"HOUSEHOLDTYPOLOGY\",\n",
    "        \"SOW_type_colr\",\n",
    "    ]\n",
    "    \n",
    "    # Only keep columns that exist\n",
    "    final_numeric = [c for c in final_numeric if c in df.columns]\n",
    "    final_categorical = [c for c in final_categorical if c in df.columns]\n",
    "    \n",
    "    # Return only the curated features\n",
    "    return df[final_numeric + final_categorical], final_numeric, final_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ae08a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # numeric coercion (NO filling here)\n",
    "    num_cols = [c for c in df.columns if c.startswith(\"cat_\")] + [\n",
    "        \"rev_ticket\", \"prod_ticket\", \"n_cogo\", \"cogo_rev\",\n",
    "        \"total_discount\", \"price_sens_colr\", \"SOW_colr\"\n",
    "    ]\n",
    "    num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # SOW normalization + flag\n",
    "    if \"SOW_type_colr\" in df.columns:\n",
    "        df[\"SOW_!\"] = (df[\"SOW_type_colr\"] == \"!\").astype(int)\n",
    "        df[\"SOW_type_colr\"] = (\n",
    "            df[\"SOW_type_colr\"]\n",
    "            .fillna(\"unknown\")\n",
    "            .replace(\"!\", \"unknown\")\n",
    "        )\n",
    "\n",
    "    # Household normalization\n",
    "    if \"HOUSEHOLDTYPOLOGY\" in df.columns:\n",
    "        df[\"HOUSEHOLDTYPOLOGY\"] = (\n",
    "            df[\"HOUSEHOLDTYPOLOGY\"]\n",
    "            .fillna(\"unknown\")\n",
    "            .replace(\"!\", \"unknown\")\n",
    "        )\n",
    "\n",
    "    # Negative value flags\n",
    "    for c in [col for col in df.columns if col.startswith(\"cat_\")]:\n",
    "        negs = df[c] < 0\n",
    "        df[f\"{c}_neg_flag\"] = negs.astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11415cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ClipLog(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Log1p transform with negative clipping. Supports get_feature_names_out.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Store feature names if available\n",
    "        if hasattr(X, 'columns'):\n",
    "            self._feature_names = list(X.columns)\n",
    "        elif hasattr(X, 'shape'):\n",
    "            self._feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        arr = np.array(X, dtype=float, copy=True)\n",
    "        arr[arr < 0] = 0\n",
    "        return np.log1p(arr)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Required for sklearn pipeline feature name propagation.\"\"\"\n",
    "        if input_features is not None:\n",
    "            return np.array(input_features)\n",
    "        return np.array(self._feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b402dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #making categories \n",
    "# # Spend / turnover-like features\n",
    "# spend_cols = [col for col in X_train.columns if col.startswith(\"cat_\")] + [\n",
    "#     \"cogo_rev\",\n",
    "#     \"total_discount\",\n",
    "#     \"rev_ticket\"\n",
    "# ]\n",
    "\n",
    "# # Count-like features (can also be scaled)\n",
    "# count_cols = [\n",
    "#     \"prod_ticket\",\n",
    "#     \"n_cogo\"\n",
    "# ]\n",
    "\n",
    "# # Other numeric features\n",
    "# other_numeric_cols = [\n",
    "#     \"price_sens_colr\",\n",
    "#     \"SOW_colr\"\n",
    "# ]\n",
    "\n",
    "# numeric_cols = spend_cols + count_cols + other_numeric_cols\n",
    "\n",
    "# categorical_cols = [\n",
    "#     \"HOUSEHOLDTYPOLOGY\",\n",
    "#     \"SOW_type_colr\"\n",
    "# ]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def infer_feature_groups(df: pd.DataFrame,\n",
    "                          skew_thresh=1.0,\n",
    "                          range_ratio_thresh=20):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    # ---- Binary (0/1) ----\n",
    "    binary_cols = []\n",
    "    for c in numeric_cols:\n",
    "        vals = df[c].dropna().unique()\n",
    "        if len(vals) <= 2 and set(vals).issubset({0, 1}):\n",
    "            binary_cols.append(c)\n",
    "\n",
    "    # ---- Bounded [0,1] but not binary we want to keep ----\n",
    "    bounded_cols = []\n",
    "    for c in numeric_cols:\n",
    "        if c in binary_cols:\n",
    "            continue\n",
    "        col = df[c].dropna()\n",
    "        if len(col) and col.min() >= 0 and col.max() <= 1:\n",
    "            bounded_cols.append(c)\n",
    "\n",
    "    log_cols = [\n",
    "        c for c in numeric_cols\n",
    "        if c not in binary_cols\n",
    "        and c not in bounded_cols\n",
    "        \n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"log_numeric\": log_cols,\n",
    "        \"binary\": binary_cols,\n",
    "        \"categorical\": categorical_cols,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a62f9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpipeline(X_train: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Senior Data Scientist Pipeline\n",
    "    \n",
    "    Key design choices:\n",
    "    - ElasticNet regularization (handles correlated features properly)\n",
    "    - Log-transform for skewed spend features\n",
    "    - StandardScaler for all numeric (critical for regularization)\n",
    "    - OneHotEncoder for categoricals\n",
    "    \n",
    "    ElasticNet rationale:\n",
    "    - L2 component stabilizes correlated features\n",
    "    - L1 component performs feature selection if redundant\n",
    "    - Best of both worlds for overlapping features\n",
    "    \"\"\"\n",
    "    # Apply feature engineering first\n",
    "    X_engineered, numeric_cols, categorical_cols = final_features(X_train)\n",
    "    \n",
    "    # Separate log-transform candidates (spend/revenue features) from others\n",
    "    log_transform_cols = [\n",
    "        \"total_spend\",\n",
    "        \"cogo_rev\",\n",
    "        \"cat_Wijn_Stillewijnen_RAYON\",\n",
    "        \"wine_affinity_spend\",\n",
    "        \"rev_ticket\",\n",
    "    ]\n",
    "    log_transform_cols = [c for c in log_transform_cols if c in numeric_cols]\n",
    "    \n",
    "    # Non-log numeric (ratios, counts, scores - already bounded or normalized)\n",
    "    standard_numeric_cols = [c for c in numeric_cols if c not in log_transform_cols]\n",
    "    \n",
    "    # Log-transform pipeline (for skewed spend features)\n",
    "    log_numeric_pipeline = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "        (\"clip_log\", ClipLog()),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Standard numeric pipeline (for ratios, scores, counts)\n",
    "    standard_numeric_pipeline = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Categorical pipeline\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    \n",
    "    categorical_pipeline = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ])\n",
    "    \n",
    "    # Column transformer with curated features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"log_num\", log_numeric_pipeline, log_transform_cols),\n",
    "            (\"std_num\", standard_numeric_pipeline, standard_numeric_cols),\n",
    "            (\"cat\", categorical_pipeline, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "    \n",
    "    # Full model pipeline with ElasticNet\n",
    "    # Senior choice: ElasticNet handles overlapping features properly\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(\n",
    "            penalty=\"elasticnet\",\n",
    "            solver=\"saga\",          # Only solver that supports elasticnet\n",
    "            l1_ratio=0.5,           # Balance between L1 and L2\n",
    "            C=1.0,                  # Regularization strength (will tune)\n",
    "            max_iter=5000,          # saga needs more iterations\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    feature_groups = {\n",
    "        \"log_numeric\": log_transform_cols,\n",
    "        \"standard_numeric\": standard_numeric_cols,\n",
    "        \"categorical\": categorical_cols,\n",
    "    }\n",
    "    \n",
    "    return model, feature_groups, X_engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "376f2739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered features: 15 total\n",
      "  Numeric: 13\n",
      "  Categorical: 2\n",
      "\n",
      "Numeric features: ['total_spend', 'rev_ticket', 'prod_ticket', 'n_cogo', 'cogo_rev', 'online_ratio', 'price_sens_colr', 'discount_ratio', 'cat_Wijn_Stillewijnen_RAYON', 'wine_affinity_spend', 'wine_affinity_ratio', 'premium_ratio', 'SOW_colr']\n",
      "Categorical features: ['HOUSEHOLDTYPOLOGY', 'SOW_type_colr']\n",
      "\n",
      "============================================================\n",
      "FEATURE GROUPS\n",
      "============================================================\n",
      "\n",
      "LOG_NUMERIC (5):\n",
      "   total_spend\n",
      "   cogo_rev\n",
      "   cat_Wijn_Stillewijnen_RAYON\n",
      "   wine_affinity_spend\n",
      "   rev_ticket\n",
      "\n",
      "STANDARD_NUMERIC (8):\n",
      "   prod_ticket\n",
      "   n_cogo\n",
      "   online_ratio\n",
      "   price_sens_colr\n",
      "   discount_ratio\n",
      "   wine_affinity_ratio\n",
      "   premium_ratio\n",
      "   SOW_colr\n",
      "\n",
      "CATEGORICAL (2):\n",
      "   HOUSEHOLDTYPOLOGY\n",
      "   SOW_type_colr\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering to train and test sets\n",
    "X_train_eng, num_cols, cat_cols = final_features(X_train)\n",
    "X_test_eng, _, _ = final_features(X_test)\n",
    "\n",
    "print(f\"Engineered features: {X_train_eng.shape[1]} total\")\n",
    "print(f\"  Numeric: {len(num_cols)}\")\n",
    "print(f\"  Categorical: {len(cat_cols)}\")\n",
    "print(f\"\\nNumeric features: {num_cols}\")\n",
    "print(f\"Categorical features: {cat_cols}\")\n",
    "\n",
    "# Build the pipeline with ElasticNet\n",
    "gold_model, feature_groups, _ = finalpipeline(X_train)\n",
    "\n",
    "# Show feature groups\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE GROUPS\")\n",
    "print(\"=\"*60)\n",
    "for k, v in feature_groups.items():\n",
    "    print(f\"\\n{k.upper()} ({len(v)}):\")\n",
    "    for c in v:\n",
    "        print(\"  \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "245fac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: STABILITY CHECK (Fixed Parameters)\n",
      "============================================================\n",
      "Running CV with fixed params to check variance before tuning...\n",
      "\n",
      "Average Precision: 0.4098 ¬± 0.0096\n",
      "F1 Score:          0.3123 ¬± 0.0019\n",
      "Precision:         0.1924 ¬± 0.0011\n",
      "Recall:            0.8295 ¬± 0.0099\n",
      "\n",
      "‚úÖ Model is VERY STABLE (std < 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, average_precision_score, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: STABILITY CHECK (Senior approach - fixed params first)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: STABILITY CHECK (Fixed Parameters)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Running CV with fixed params to check variance before tuning...\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Use average_precision (better for imbalanced data than F1)\n",
    "stability_scores = cross_validate(\n",
    "    gold_model,\n",
    "    X_train_eng,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'average_precision': 'average_precision',\n",
    "        'f1': 'f1',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall'\n",
    "    },\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\nAverage Precision: {stability_scores['test_average_precision'].mean():.4f} ¬± {stability_scores['test_average_precision'].std():.4f}\")\n",
    "print(f\"F1 Score:          {stability_scores['test_f1'].mean():.4f} ¬± {stability_scores['test_f1'].std():.4f}\")\n",
    "print(f\"Precision:         {stability_scores['test_precision'].mean():.4f} ¬± {stability_scores['test_precision'].std():.4f}\")\n",
    "print(f\"Recall:            {stability_scores['test_recall'].mean():.4f} ¬± {stability_scores['test_recall'].std():.4f}\")\n",
    "\n",
    "# Check if variance is acceptable (std < 0.05 is good)\n",
    "ap_std = stability_scores['test_average_precision'].std()\n",
    "if ap_std < 0.03:\n",
    "    print(\"\\n‚úÖ Model is VERY STABLE (std < 0.03)\")\n",
    "elif ap_std < 0.05:\n",
    "    print(\"\\n‚úÖ Model is STABLE (std < 0.05)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Model has HIGH VARIANCE (std = {ap_std:.4f}) - proceed with caution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8e5468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: COEFFICIENT STABILITY ACROSS FOLDS\n",
      "============================================================\n",
      "\n",
      "Total features: 39\n",
      "Stable features (same sign in ‚â•80% folds): 36\n",
      "Unstable features (sign flips): 3\n",
      "\n",
      "‚ö†Ô∏è Unstable features (coefficients flip sign):\n",
      "   HOUSEHOLDTYPOLOGY_f_HHnochild_35_54: stable in 60% of folds\n",
      "   HOUSEHOLDTYPOLOGY_k_HHchild_oldest_13_17: stable in 40% of folds\n",
      "   SOW_type_colr_unknown: stable in 60% of folds\n",
      "\n",
      "------------------------------------------------------------\n",
      "TOP 10 FEATURES BY MEAN |COEFFICIENT|\n",
      "------------------------------------------------------------\n",
      "  + rev_ticket                               coef=+1.980 ¬± 0.019 (stable: 100%)\n",
      "  + wine_affinity_spend                      coef=+1.755 ¬± 0.046 (stable: 100%)\n",
      "  - SOW_type_colr_SOW00-10                   coef=-1.374 ¬± 0.043 (stable: 100%)\n",
      "  + SOW_type_colr_Outlier_fr                 coef=+1.331 ¬± 0.081 (stable: 100%)\n",
      "  - discount_ratio                           coef=-1.097 ¬± 0.096 (stable: 100%)\n",
      "  - SOW_type_colr_SOW10-20                   coef=-1.087 ¬± 0.035 (stable: 100%)\n",
      "  - total_spend                              coef=-1.035 ¬± 0.039 (stable: 100%)\n",
      "  - prod_ticket                              coef=-0.830 ¬± 0.011 (stable: 100%)\n",
      "  - SOW_type_colr_SOW20-30                   coef=-0.788 ¬± 0.029 (stable: 100%)\n",
      "  + SOW_type_colr_Outlier_om                 coef=+0.783 ¬± 0.071 (stable: 100%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: COEFFICIENT STABILITY ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: COEFFICIENT STABILITY ACROSS FOLDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def get_feature_names(model, feature_groups):\n",
    "    \"\"\"Simple helper to get feature names from our pipeline.\"\"\"\n",
    "    names = feature_groups['log_numeric'] + feature_groups['standard_numeric']\n",
    "    # Add OHE categories\n",
    "    ohe = model.named_steps['preprocess'].named_transformers_['cat'].named_steps['ohe']\n",
    "    for col_idx, col in enumerate(feature_groups['categorical']):\n",
    "        for cat in ohe.categories_[col_idx]:\n",
    "            names.append(f\"{col}_{cat}\")\n",
    "    return names\n",
    "\n",
    "# Collect coefficients from each fold\n",
    "fold_coefs = []\n",
    "fold_feature_names = None\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train_eng, y_train)):\n",
    "    X_fold_train = X_train_eng.iloc[train_idx]\n",
    "    y_fold_train = y_train.iloc[train_idx]\n",
    "    \n",
    "    fold_model = clone(gold_model)\n",
    "    fold_model.fit(X_fold_train, y_fold_train)\n",
    "    fold_coefs.append(fold_model.named_steps['classifier'].coef_[0])\n",
    "    \n",
    "    if fold_feature_names is None:\n",
    "        fold_feature_names = get_feature_names(fold_model, feature_groups)\n",
    "\n",
    "fold_coefs = np.array(fold_coefs)\n",
    "\n",
    "# Check sign stability\n",
    "sign_stability = (np.sign(fold_coefs) == np.sign(fold_coefs.mean(axis=0))).mean(axis=0)\n",
    "unstable_mask = sign_stability < 0.8\n",
    "\n",
    "print(f\"\\nTotal features: {len(fold_feature_names)}\")\n",
    "print(f\"Stable features (same sign ‚â•80% folds): {(~unstable_mask).sum()}\")\n",
    "print(f\"Unstable features (sign flips): {unstable_mask.sum()}\")\n",
    "\n",
    "if unstable_mask.any():\n",
    "    print(\"\\n‚ö†Ô∏è Unstable features:\")\n",
    "    for name, stab in zip(fold_feature_names, sign_stability):\n",
    "        if stab < 0.8:\n",
    "            print(f\"   {name}: stable in {stab*100:.0f}% of folds\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All features have stable coefficient signs!\")\n",
    "\n",
    "# Top features\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"TOP 10 FEATURES BY MEAN |COEFFICIENT|\")\n",
    "print(\"-\"*60)\n",
    "mean_coefs = fold_coefs.mean(axis=0)\n",
    "std_coefs = fold_coefs.std(axis=0)\n",
    "top_idx = np.argsort(np.abs(mean_coefs))[-10:][::-1]\n",
    "\n",
    "for idx in top_idx:\n",
    "    sign = \"+\" if mean_coefs[idx] > 0 else \"-\"\n",
    "    print(f\"  {sign} {fold_feature_names[idx]:40s} coef={mean_coefs[idx]:+.3f} ¬± {std_coefs[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98aa4d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: HYPOTHESIS-DRIVEN TUNING\n",
      "============================================================\n",
      "Senior approach: Small grid, hypothesis-driven, not slot machine\n",
      "\n",
      "Grid size: 12 combinations\n",
      "Scoring: average_precision (better for imbalanced marketing problems)\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/pranavreddy/Desktop/projects/wines/Highbrow-wines-case-predictive-modelling-/.venv-py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Best Average Precision: 0.4098\n",
      "   Best C: 2.0\n",
      "   Best l1_ratio: 0.5\n",
      "\n",
      "üìä Improvement from tuning: +0.01%\n",
      "   ‚Üí Minimal improvement. Default params are fine.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: SMALL HYPOTHESIS-DRIVEN GRID SEARCH\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: HYPOTHESIS-DRIVEN TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Senior approach: Small grid, hypothesis-driven, not slot machine\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Small, focused grid (not 200 combinations)\n",
    "# Hypothesis: ElasticNet with moderate regularization should work best\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 0.5, 1.0, 2.0],      # Regularization strength\n",
    "    'classifier__l1_ratio': [0.3, 0.5, 0.7],    # L1 vs L2 balance\n",
    "}\n",
    "\n",
    "print(f\"\\nGrid size: {len(param_grid['classifier__C']) * len(param_grid['classifier__l1_ratio'])} combinations\")\n",
    "print(\"Scoring: average_precision (better for imbalanced marketing problems)\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gold_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='average_precision',  # Senior choice for imbalanced data\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_eng, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best Average Precision: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   Best C: {grid_search.best_params_['classifier__C']}\")\n",
    "print(f\"   Best l1_ratio: {grid_search.best_params_['classifier__l1_ratio']}\")\n",
    "\n",
    "# Compare default vs tuned\n",
    "baseline_ap = stability_scores['test_average_precision'].mean()\n",
    "tuned_ap = grid_search.best_score_\n",
    "improvement = (tuned_ap - baseline_ap) / baseline_ap * 100\n",
    "\n",
    "print(f\"\\nüìä Improvement from tuning: {improvement:+.2f}%\")\n",
    "if improvement < 2:\n",
    "    print(\"   ‚Üí Minimal improvement. Default params are fine.\")\n",
    "elif improvement < 5:\n",
    "    print(\"   ‚Üí Modest improvement. Use tuned params.\")\n",
    "else:\n",
    "    print(\"   ‚Üí Significant improvement. Tuning was worthwhile.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8592da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: THRESHOLD TUNING\n",
      "============================================================\n",
      "Senior insight: Tune the decision threshold, not just model weights\n",
      "\n",
      "üìä Threshold Analysis:\n",
      "------------------------------------------------------------\n",
      "Max F1 threshold:           0.834 (F1=0.441)\n",
      "Max Recall @ Precision‚â•70%: 0.986 (Recall=0.127)\n",
      "Max Recall @ Precision‚â•50%: 0.914 (Recall=0.328)\n",
      "Max Recall @ Precision‚â•30%: 0.721 (Recall=0.630)\n",
      "\n",
      "üéØ Using threshold: 0.834 (Max F1 objective)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: THRESHOLD TUNING (Senior approach: tune threshold, not just weights)\n",
    "# =============================================================================\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4: THRESHOLD TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Senior insight: Tune the decision threshold, not just model weights\")\n",
    "\n",
    "# Get best model and predict probabilities\n",
    "best_model = grid_search.best_estimator_\n",
    "y_proba = best_model.predict_proba(X_test_eng)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Find optimal threshold for different business objectives\n",
    "print(\"\\nüìä Threshold Analysis:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Objective 1: Maximize F1\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "best_f1_idx = np.argmax(f1_scores[:-1])  # exclude last element\n",
    "best_f1_threshold = thresholds[best_f1_idx]\n",
    "print(f\"Max F1 threshold:           {best_f1_threshold:.3f} (F1={f1_scores[best_f1_idx]:.3f})\")\n",
    "\n",
    "# Objective 2: Recall @ Precision >= 0.7 (realistic for marketing)\n",
    "mask_p70 = precision[:-1] >= 0.7\n",
    "if mask_p70.any():\n",
    "    best_recall_at_p70 = recall[:-1][mask_p70].max()\n",
    "    idx_p70 = np.where((recall[:-1] == best_recall_at_p70) & mask_p70)[0][0]\n",
    "    threshold_p70 = thresholds[idx_p70]\n",
    "    print(f\"Max Recall @ Precision‚â•70%: {threshold_p70:.3f} (Recall={best_recall_at_p70:.3f})\")\n",
    "else:\n",
    "    print(\"Max Recall @ Precision‚â•70%: Not achievable\")\n",
    "#objective 3: Recall @ Precision >= 0.5 (balanced)\n",
    "mask_p50 = precision[:-1] >= 0.5\n",
    "if mask_p50.any():\n",
    "    best_recall_at_p50 = recall[:-1][mask_p50].max()\n",
    "    idx_p50 = np.where((recall[:-1] == best_recall_at_p50) & mask_p50)[0][0]\n",
    "    threshold_p50 = thresholds[idx_p50]\n",
    "    print(f\"Max Recall @ Precision‚â•50%: {threshold_p50:.3f} (Recall={best_recall_at_p50:.3f})\")\n",
    "else:\n",
    "    print(\"Max Recall @ Precision‚â•50%: Not achievable\")\n",
    "\n",
    "# Objective 4: Recall @ Precision >= 0.3 (broader reach)\n",
    "mask_p30 = precision[:-1] >= 0.3\n",
    "if mask_p30.any():\n",
    "    best_recall_at_p30 = recall[:-1][mask_p30].max()\n",
    "    idx_p30 = np.where((recall[:-1] == best_recall_at_p30) & mask_p30)[0][0]\n",
    "    threshold_p30 = thresholds[idx_p30]\n",
    "    print(f\"Max Recall @ Precision‚â•30%: {threshold_p30:.3f} (Recall={best_recall_at_p30:.3f})\")\n",
    "else:\n",
    "    print(\"Max Recall @ Precision‚â•30%: Not achievable\")\n",
    "\n",
    "# Use best F1 threshold for final evaluation\n",
    "CHOSEN_THRESHOLD = best_f1_threshold\n",
    "print(f\"\\nüéØ Using threshold: {CHOSEN_THRESHOLD:.3f} (Max F1 objective)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1af3d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5: FINAL TEST SET EVALUATION\n",
      "============================================================\n",
      "\n",
      "üìä Classification Report (Tuned Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Highbrow       0.97      0.96      0.97     38008\n",
      "    Highbrow       0.40      0.49      0.44      1992\n",
      "\n",
      "    accuracy                           0.94     40000\n",
      "   macro avg       0.69      0.73      0.70     40000\n",
      "weighted avg       0.94      0.94      0.94     40000\n",
      "\n",
      "\n",
      "üìä Confusion Matrix (Tuned Threshold):\n",
      "                    Predicted\n",
      "                No         Yes\n",
      "Actual No    36550        1458\n",
      "Actual Yes    1017         975\n",
      "\n",
      "üìä Business Metrics:\n",
      "   True Positives (correctly identified buyers): 975\n",
      "   False Positives (wasted marketing): 1458\n",
      "   False Negatives (missed buyers): 1017\n",
      "   True Negatives (correctly ignored): 36550\n",
      "\n",
      "   üìß If you target 2433 customers:\n",
      "      ‚Üí 975 will buy (40.1% hit rate)\n",
      "      ‚Üí 1458 won't buy (wasted effort)\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìä THRESHOLD COMPARISON:\n",
      "------------------------------------------------------------\n",
      "Default (0.50): Precision=0.190, Recall=0.824, TP=1642, FP=6982\n",
      "Tuned (0.83):   Precision=0.401, Recall=0.489, TP=975, FP=1458\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: FINAL TEST SET EVALUATION\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 5: FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predictions with tuned threshold\n",
    "y_pred_tuned = (y_proba >= CHOSEN_THRESHOLD).astype(int)\n",
    "y_pred_default = best_model.predict(X_test_eng)  # default 0.5 threshold\n",
    "\n",
    "print(\"\\nüìä Classification Report (Tuned Threshold):\")\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=[\"No Highbrow\", \"Highbrow\"]))\n",
    "\n",
    "print(\"\\nüìä Confusion Matrix (Tuned Threshold):\")\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"                    Predicted\")\n",
    "print(f\"                No         Yes\")\n",
    "print(f\"Actual No    {tn:5d}       {fp:5d}\")\n",
    "print(f\"Actual Yes   {fn:5d}       {tp:5d}\")\n",
    "\n",
    "print(\"\\nüìä Business Metrics:\")\n",
    "print(f\"   True Positives (correctly identified buyers): {tp}\")\n",
    "print(f\"   False Positives (wasted marketing): {fp}\")\n",
    "print(f\"   False Negatives (missed buyers): {fn}\")\n",
    "print(f\"   True Negatives (correctly ignored): {tn}\")\n",
    "\n",
    "# Marketing efficiency\n",
    "if tp + fp > 0:\n",
    "    precision_val = tp / (tp + fp)\n",
    "    print(f\"\\n   üìß If you target {tp + fp} customers:\")\n",
    "    print(f\"      ‚Üí {tp} will buy ({precision_val*100:.1f}% hit rate)\")\n",
    "    print(f\"      ‚Üí {fp} won't buy (wasted effort)\")\n",
    "\n",
    "# Compare default vs tuned threshold\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"üìä THRESHOLD COMPARISON:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "cm_default = confusion_matrix(y_test, y_pred_default)\n",
    "tn_d, fp_d, fn_d, tp_d = cm_default.ravel()\n",
    "\n",
    "print(f\"Default (0.50): Precision={tp_d/(tp_d+fp_d):.3f}, Recall={tp_d/(tp_d+fn_d):.3f}, TP={tp_d}, FP={fp_d}\")\n",
    "print(f\"Tuned ({CHOSEN_THRESHOLD:.2f}):   Precision={precision_val:.3f}, Recall={tp/(tp+fn):.3f}, TP={tp}, FP={fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c92583c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6: FEATURE IMPORTANCE INTERPRETATION\n",
      "============================================================\n",
      "Senior rule: Interpret coefficients as 'holding X constant...'\n",
      "\n",
      "Total features: 39\n",
      "Feature names extracted: 39\n",
      "\n",
      "üîù TOP 15 FEATURES BY IMPORTANCE:\n",
      "------------------------------------------------------------\n",
      "  ‚Üë rev_ticket                                    +1.9810\n",
      "  ‚Üë wine_affinity_spend                           +1.7559\n",
      "  ‚Üì SOW_type_colr_SOW00-10                        -1.4012\n",
      "  ‚Üë SOW_type_colr_Outlier_fr                      +1.3475\n",
      "  ‚Üì SOW_type_colr_SOW10-20                        -1.1087\n",
      "  ‚Üì discount_ratio                                -1.0940\n",
      "  ‚Üì total_spend                                   -1.0377\n",
      "  ‚Üë SOW_type_colr_Outlier_om                      +0.8425\n",
      "  ‚Üì prod_ticket                                   -0.8292\n",
      "  ‚Üì SOW_type_colr_SOW20-30                        -0.8053\n",
      "  ‚Üì wine_affinity_ratio                           -0.6657\n",
      "  ‚Üì HOUSEHOLDTYPOLOGY_i_HHchild_oldest_0_5        -0.5614\n",
      "  ‚Üì HOUSEHOLDTYPOLOGY_a_Single_18_34              -0.5389\n",
      "  ‚Üë HOUSEHOLDTYPOLOGY_e_HHnochild_18_34           +0.4930\n",
      "  ‚Üì SOW_type_colr_SOW30-40                        -0.4872\n",
      "\n",
      "üìñ INTERPRETATION GUIDE:\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Positive coefficient: Feature INCREASES likelihood of buying highbrow wine\n",
      "‚Ä¢ Negative coefficient: Feature DECREASES likelihood of buying highbrow wine\n",
      "‚Ä¢ 'Holding other features constant' is implicit in interpretation\n",
      "\n",
      "‚úÖ KEY POSITIVE DRIVERS (buy highbrow):\n",
      "   ‚Ä¢ rev_ticket\n",
      "   ‚Ä¢ wine_affinity_spend\n",
      "   ‚Ä¢ SOW_type_colr_Outlier_fr\n",
      "   ‚Ä¢ SOW_type_colr_Outlier_om\n",
      "   ‚Ä¢ HOUSEHOLDTYPOLOGY_e_HHnochild_18_34\n",
      "\n",
      "‚ùå KEY NEGATIVE DRIVERS (don't buy highbrow):\n",
      "   ‚Ä¢ SOW_type_colr_SOW00-10\n",
      "   ‚Ä¢ SOW_type_colr_SOW10-20\n",
      "   ‚Ä¢ discount_ratio\n",
      "   ‚Ä¢ total_spend\n",
      "   ‚Ä¢ prod_ticket\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: FEATURE IMPORTANCE (Senior interpretation)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 6: FEATURE IMPORTANCE INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Senior rule: Interpret coefficients as 'holding X constant...'\")\n",
    "\n",
    "# Get feature names and coefficients\n",
    "feature_names = get_feature_names(best_model, feature_groups)\n",
    "coefs = best_model.named_steps['classifier'].coef_[0]\n",
    "\n",
    "print(f\"\\nTotal features: {len(coefs)}\")\n",
    "\n",
    "# Create sorted DataFrame\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefs,\n",
    "}).assign(abs_coef=lambda x: np.abs(x.coefficient)).sort_values('abs_coef', ascending=False)\n",
    "\n",
    "print(\"\\nüîù TOP 15 FEATURES BY IMPORTANCE:\")\n",
    "print(\"-\"*60)\n",
    "for _, row in coef_df.head(15).iterrows():\n",
    "    direction = \"‚Üë\" if row['coefficient'] > 0 else \"‚Üì\"\n",
    "    print(f\"  {direction} {row['feature']:40s} {row['coefficient']:+.4f}\")\n",
    "\n",
    "print(\"\\nüìñ INTERPRETATION:\")\n",
    "print(\"‚Ä¢ ‚Üë Positive = INCREASES likelihood of buying highbrow wine\")\n",
    "print(\"‚Ä¢ ‚Üì Negative = DECREASES likelihood of buying highbrow wine\")\n",
    "\n",
    "print(\"\\n‚úÖ KEY POSITIVE DRIVERS:\")\n",
    "for f in coef_df[coef_df['coefficient'] > 0].head(5)['feature']:\n",
    "    print(f\"   ‚Ä¢ {f}\")\n",
    "\n",
    "print(\"\\n‚ùå KEY NEGATIVE DRIVERS:\")\n",
    "for f in coef_df[coef_df['coefficient'] < 0].head(5)['feature']:\n",
    "    print(f\"   ‚Ä¢ {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892e513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27265b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6693bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2ec01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c7e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.2\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
